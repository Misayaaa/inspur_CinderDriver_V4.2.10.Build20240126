# Copyright 2017 inspur Corp.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
"""
RDMA volume driver for inspur InStorage family and MCS storage systems.

Notes:
1. Make sure you config the password or key file. If you specify both
a password and a key file, this driver will use the key file only.
2. When a key file is used for authentication, the private key is stored
in a secure manner by the user or system administrator.
3. The defaults for creating volumes are thin volume.  These can be changed in the configuration
file or by using volume types(recommended only for advanced users).

Limitations:
1. The driver expects CLI output in English, error messages may be in a
localized format.
2. when you clone or create volumes from snapshots, it not support that the
source and target_rep are different size.

Perform necessary work to make an RDMA connection:
To be able to create an RDMA connection from a given host to a volume,
we must:
1. Translate the given nvme name to a host name
2. Create new host on the storage system if it does not yet exist
3. Map the volume to the host if it is not already done
4. Return the connection information for relevant nodes
   (in theproper I/O group)

"""
from cinder import exception
from cinder.i18n import _
from cinder.volume.drivers.inspur.instorage import instorage_common
from cinder.volume.drivers.inspur import base


# !!!ATTENTION
# the openstack common package is move out from project
# as some independent library from Kilo
try:
    from oslo.config import cfg
    from cinder.openstack.common import log as logging
    from cinder.openstack.common import excutils
except ImportError:
    from oslo_config import cfg
    from oslo_log import log as logging
    from oslo_utils import excutils

try:
    # lock from coordination is used instead of utils since Ocata
    from cinder import coordination
    if None in coordination.synchronized.__defaults__:
        from cinder import utils as coordination

        def synchronized_lock(lockname):
            return coordination.synchronized(lockname, external=True)
    else:
        def synchronized_lock(lockname):
            return coordination.synchronized(lockname)
except ImportError:
    # lock from utils before NEWTON(incuding NEWTON)
    from cinder import utils as coordination

    def synchronized_lock(lockname):
        return coordination.synchronized(lockname, external=True)


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
CONF.register_opts(base.instorage_mcs_roce_opts)


class InStorageMCSRDMADriver(instorage_common.InStorageMCSCommonDriver):
    """inspur InStorage RDMA volume driver."""

    def __init__(self, *args, **kwargs):
        super(InStorageMCSRDMADriver, self).__init__(*args, **kwargs)
        self.protocol = 'RDMA'
        self.configuration.append_config_values(
            base.instorage_mcs_roce_opts)

    def initialize_connection(self, volume, connector):
        """Perform necessary work to make an RDMA connection."""
        @synchronized_lock('instorage-host' + self._state['system_id']
                           + connector['host'])
        def _do_initialize_connection_locked():
            return self._do_initialize_connection(volume, connector)
        return _do_initialize_connection_locked()

    def initialize_connection_snapshot(self, snapshot, connector):
        """Attach a snapshot."""
        #as replication volume which only has snapshot in main,
        #so when it failed over, we can not attach that snapshot.
        volume = dict(
            id=snapshot.id,
            name=snapshot.name,
            volume_type_id=snapshot.volume_type_id
        )

        return self.initialize_connection(volume, connector)

    def terminate_connection(self, volume, connector, **kwargs):
        """Cleanup after an RDMA connection has been terminated."""
        # If a fake connector is generated by nova when the host
        # is down, then the connector will not have a host property,
        # In this case construct the lock without the host property
        # so that all the fake connectors to an MCS are serialized
        host = ''
        if connector is not None and 'host' in connector:
            service_host_attach_count = 0
            for item in volume.volume_attachment:
                if (hasattr(item, 'connector')
                    and item.connector is not None
                    and 'host' in item.connector
                    and connector['host'] == item.connector['host']):
                    service_host_attach_count += 1
            if service_host_attach_count > 1:
                return {}

            host = connector['host']

        @synchronized_lock('instorage-host' + self._state['system_id'] + host)
        def _do_terminate_connection_locked():
            return self._do_terminate_connection(volume, connector,
                                                 **kwargs)
        return _do_terminate_connection_locked()

    def terminate_connection_snapshot(self, snapshot, connector, **kwargs):
        volume = dict(
            id=snapshot.id,
            name=snapshot.name
        )

        host = ''
        if connector is not None and 'host' in connector:
            host = connector['host']

        @synchronized_lock('instorage-host' + self._state['system_id'] + host)
        def _do_terminate_connection_locked():
            return self._do_terminate_connection(volume, connector,
                                                 **kwargs)
        return _do_terminate_connection_locked()

    def _do_initialize_connection(self, volume, connector):
        """Perform necessary work to make an RDMA connection."""
        LOG.info('enter: initialize_connection: volume %(vol)s with connector'
                  ' %(conn)s', {'vol': volume['id'], 'conn': connector})

        # get host according to RDMA protocol
        connector = connector.copy()
        connector.pop('wwpns', None)
        connector.pop('wwnns', None)
        connector.pop('initiator', None)

        volume_name = self._get_target_vol(volume)

        # Check if a host object is defined for this host name
        host_name = self._assistant.get_host_from_connector(connector)
        if host_name is None:
            # Host does not exist - add a new host to InStorage/MCS
            site = self._assistant.get_volume_prefer_site_name(volume_name)
            try:
                host_name = self._assistant.create_host(connector, site)
            except exception.VolumeBackendAPIException:
                host_name = self._assistant.get_host_from_connector(connector)
                if host_name is None:
                    msg = _('Failed to create host with connector %s') % connector
                    LOG.error(msg)
                    raise exception.VolumeBackendAPIException(data=msg)

        self._assistant.map_vol_to_host(volume_name, host_name, True, 'nvme')
        multipath = connector.get('nvme_native_multipath', False)
        self._state['storage_nodes'] = self._init_node_info()
        try:
            properties = self._get_nvme_data(volume, connector, multipath)

        except Exception:
            with excutils.save_and_reraise_exception():
                self._do_terminate_connection(volume, connector)
                LOG.error(_('initialize_connection: Failed '
                            'to collect return '
                            'properties for volume %(vol)s and connector '
                            '%(conn)s.\n'), {'vol': volume, 'conn': connector})

        LOG.info('leave: initialize_connection:\n volume: %(vol)s\n '
                  'connector: %(conn)s\n properties: %(prop)s',
                  {'vol': volume['id'], 'conn': connector,
                   'prop': properties})

        return {'driver_volume_type': 'nvmeof', 'data': properties, }

    def _get_nvme_data(self, volume, connector, multipath):
        LOG.debug('enter: _get_nvme_data: volume %(vol)s with connector %(conn)s',
                  {'vol': volume['id'], 'conn': connector})
        # if NAS engine started. Storage will set up some ips for NAS cluster.
        # And this ips can not be used for iscsi. We will filter it
        filter_ips = ['172.16.2.33','172.16.2.43','172.16.2.53',
                      '172.16.2.32','172.16.2.42','172.16.2.52']
        properties = {}
        volume_name = self._get_target_vol(volume)
        volume_attributes = self._assistant.get_vdisk_attributes(volume_name)
        if volume_attributes is None:
            msg = (_('_get_nvme_data: Failed to get attributes'
                     ' for volume %s.') % volume_name)
            LOG.error(msg)
            raise exception.VolumeDriverException(message=msg)

        try:
            preferred_node = (volume_attributes.get('preferred_node_id',None)
                             or volume_attributes.get('preferredNodeId',None))
            io_group = (volume_attributes.get('IO_group_id',None)
                        or volume_attributes.get('ioGroupId',None))
        except KeyError as exp:
            msg = (_('_get_nvme_data: Did not find expected column'
                     ' name in %(volume)s: %(key)s  %(error)s.'),
                   {'volume': volume_name, 'key': exp.args[0],
                    'error': exp})
            LOG.error(msg)
            raise exception.VolumeBackendAPIException(data=msg)

        # Get preferred node and other nodes in I/O group
        preferred_node_entry = None
        io_group_nodes = []

        for node in self._state['storage_nodes'].values():
            if self.protocol not in node['enabled_protocols']:
                continue

            if node['IO_group'] != io_group:
                continue
            io_group_nodes.append(node)
            if node['id'] == preferred_node:
                preferred_node_entry = node

        if not io_group_nodes:
            msg = (_('_get_nvme_data: No node found in '
                     'I/O group %(gid)s for volume %(vol)s.') % {
                'gid': io_group, 'vol': volume_name})
            LOG.error(msg)
            raise exception.VolumeBackendAPIException(data=msg)

        if not preferred_node_entry:
            # Get 1st node in I/O group
            preferred_node_entry = io_group_nodes[0]
            LOG.warning(_('_get_nvme_data: Did not find a '
                          'preferred node for volume %s.'), volume_name)

        if preferred_node_entry['ipv4']:
            ipaddr = preferred_node_entry['ipv4'][0]
        else:
            ipaddr = '[%s]' % preferred_node_entry['ipv6'][0]
            # ipv6 need surround with brackets when it use port
        if not multipath:
            properties['target_portal'] = '%s' % ipaddr
            properties['target_port'] = 4420
            properties['transport_type'] = 'rdma'
            properties['nqn'] = self._assistant.get_storage_nqn()
            properties['volume_nguid'] = volume_attributes['vdisk_NGUID'].lower()
            LOG.info('leave: _get_nvme_data:\n volume: %(vol)s\n '
                    'connector: %(conn)s\n nguid: %(nguid)s\n '
                    'properties: %(prop)s',
                    {'vol': volume['id'],
                    'conn': connector,
                    'nguid': properties['volume_nguid'],
                    'prop': properties})
        else:
            try:
                specified_rdma_ip_list = self.configuration.instorage_mcs_rdma_ips
                resp = self._assistant.executor.lsportip()
            except Exception as ex:
                msg = (_('_get_nvme_data: Failed to '
                        'get port ip because of exception: '
                        '%s.') % six.text_type(ex))
                LOG.exception(msg)
                raise exception.VolumeBackendAPIException(data=msg)
            portals = []
            for ip_data in resp:
                link_state = ip_data.get('link_state', None) or ip_data.get('linkState', None)
                valid_port = ''
                if ((ip_data['state'] == 'configured' and
                        link_state == 'active') or
                        ip_data['state'] == 'online'):
                    vaild_port_ipv4 = (ip_data.get('IP_address',None) or
                                       ip_data.get('ipAddress',None))
                    vaild_port_ipv6 = (ip_data.get('IP_address_6',None) or
                                       ip_data.get('ipAddress6',None))
                    if (specified_rdma_ip_list and
                        (vaild_port_ipv4 not in specified_rdma_ip_list and
                        vaild_port_ipv6 not in specified_rdma_ip_list)):
                        continue
                    if vaild_port_ipv4:
                        valid_port = '%s' % vaild_port_ipv4
                    elif vaild_port_ipv6:
                        valid_port = '[%s]' % vaild_port_ipv6
                if valid_port and valid_port not in filter_ips:
                    portals.append((valid_port, 4420, 'rdma'))
            properties['target_nqn'] = self._assistant.get_storage_nqn()
            properties['portals'] = portals
            properties['volume_nguid'] = volume_attributes['vdisk_NGUID'].lower()
            LOG.info('leave: _get_nvme_data with nvme-multipathing:\n volume: %(vol)s\n '
                    'connector: %(conn)s\n nguid: %(nguid)s\n '
                    'properties: %(prop)s',
                    {'vol': volume['id'],
                    'conn': connector,
                    'nguid': properties['volume_nguid'],
                    'prop': properties})


        return properties

    def _do_terminate_connection(self, volume, connector, **kwargs):
        """Cleanup after an RDMA connection has been terminated.

        When we clean up a terminated connection between a given connector
        and volume, we:
        1. Translate the given connector to a host name
        2. Remove the volume-to-host mapping if it exists
        3. Delete the host if it has no more mappings (hosts are created
        automatically by this driver when mappings are created)
        """
        LOG.debug('enter: terminate_connection: volume %(vol)s with connector'
                  ' %(conn)s', {'vol': volume['id'], 'conn': connector})
        vol_name = self._get_target_vol(volume)

        info = {}
        if 'host' in connector:
            # get host according to RDMA protocol
            connector = connector.copy()
            connector.pop('wwpns', None)
            connector.pop('wwnns', None)
            connector.pop('initiator', None)

            info = {'driver_volume_type': 'nvmeof',
                    'data': {}}

            host_name = self._assistant.get_host_from_connector(connector)
            if host_name is None:
                msg = (_('terminate_connection: Failed to get host name from'
                         ' connector.'))
                LOG.warning(msg)
                return info
        else:
            host_name = None

        # Unmap volumes, if hostname is None, need to get value from vdiskmap
        self._assistant.unmap_vol_from_host(vol_name, host_name)

        LOG.debug('leave: terminate_connection: volume %(vol)s with '
                  'connector %(conn)s', {'vol': volume['id'],
                                         'conn': connector})
        return info
